{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#저장:현재 작업 디렉토리\n",
        "import os\n",
        "SAVE_DIR = '.'\n",
        "current_directory = os.getcwd()\n",
        "print(current_directory)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k8xN7nrw_19r",
        "outputId": "d0ad14ee-0127-466f-b7b6-57220dfca962"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. 모델 정의 (VGG16_BN)"
      ],
      "metadata": {
        "id": "cAX5aVno2h33"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 필요한 라이브러리\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import math\n",
        "from collections import OrderedDict"
      ],
      "metadata": {
        "id": "k6B_pvuw2bux"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#------------------------------신경망 구조 설정값(vgg_16_bn)모델 전용-------------------------------------\n",
        "defaultcfg = [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M', 512, 512, 512, 512]\n",
        "relucfg = [2, 6, 9, 13, 16, 19, 23, 26, 29, 33, 36, 39]\n",
        "convcfg = [0, 3, 7, 10, 14, 17, 20, 24, 27, 30, 34, 37]\n",
        "#---------------------------------------------------------------------------------------------------------\n",
        "\n",
        "\n",
        "class VGG(nn.Module):\n",
        "    def __init__(self, num_classes=100, init_weights=True, cfg=None, compress_rate=None):\n",
        "        super(VGG, self).__init__()\n",
        "        self.features = nn.Sequential()\n",
        "\n",
        "        if cfg is None:\n",
        "            cfg = defaultcfg\n",
        "\n",
        "        #(추가) compress_rate가 None이면 기본값 설정. 모델 훈련시에는 None입력\n",
        "        if compress_rate is None:\n",
        "          num_conv = len([x for x in cfg[:-1] if x != 'M'])\n",
        "          compress_rate = [0.0] * num_conv\n",
        "\n",
        "        self.relucfg = relucfg\n",
        "        self.covcfg = convcfg\n",
        "        self.compress_rate = compress_rate\n",
        "        self.features = self.make_layers(cfg[:-1], True, compress_rate)\n",
        "        #(추가) CIFAR100데이터셋에서 Regularization을 위해 기존 classifier구조에\n",
        "        # Dropout layer를 추가함.\n",
        "        self.classifier = nn.Sequential(OrderedDict([\n",
        "            ('linear1', nn.Linear(cfg[-2], cfg[-1])),\n",
        "            ('norm1', nn.BatchNorm1d(cfg[-1])),\n",
        "            ('relu1', nn.ReLU(inplace=True)),\n",
        "            ('dropout1', nn.Dropout(0.5)), #(추가)\n",
        "            ('linear2', nn.Linear(cfg[-1], num_classes)),\n",
        "        ]))\n",
        "\n",
        "        if init_weights:\n",
        "            self._initialize_weights()\n",
        "\n",
        "    def make_layers(self, cfg, batch_norm=True, compress_rate=None):\n",
        "        layers = nn.Sequential()\n",
        "        in_channels = 3\n",
        "        cnt = 0\n",
        "        for i, v in enumerate(cfg):\n",
        "            if v == 'M':\n",
        "                layers.add_module('pool%d' % i, nn.MaxPool2d(kernel_size=2, stride=2))\n",
        "            else:\n",
        "                conv2d = nn.Conv2d(in_channels, v, kernel_size=3, padding=1)\n",
        "                conv2d.cp_rate = compress_rate[cnt]\n",
        "                cnt += 1\n",
        "\n",
        "                layers.add_module('conv%d' % i, conv2d)\n",
        "                layers.add_module('norm%d' % i, nn.BatchNorm2d(v))\n",
        "                layers.add_module('relu%d' % i, nn.ReLU(inplace=True))\n",
        "                in_channels = v\n",
        "\n",
        "        return layers\n",
        "\n",
        "    def forward(self,x):\n",
        "      x = self.features(x)\n",
        "\n",
        "      x = nn.AvgPool2d(2)(x)\n",
        "      x = x.view(x.size(0),-1)\n",
        "      x = self.classifier(x)\n",
        "      return x\n",
        "\n",
        "    def _initialize_weights(self):\n",
        "      for m in self.modules():\n",
        "        if isinstance(m,nn.Conv2d):\n",
        "          n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels #(n = H x W x out_channels).He초기화에 사용\n",
        "          m.weight.data.normal_(0,math.sqrt(2. /n)) ## He 초기화, (0,0sqrt(2/n))정규분포에서 샘플링\n",
        "          if m.bias is not None:\n",
        "            m.bias.data.zero_()\n",
        "        elif isinstance(m,nn.BatchNorm2d):\n",
        "          m.weight.data.fill_(0.5)\n",
        "          m.bias.data.zero_()\n",
        "        elif isinstance(m,nn.Linear):\n",
        "          m.weight.data.normal_(0,0.01)\n",
        "          m.bias.data.zero_()\n",
        "\n",
        "def vgg_16_bn(compress_rate = None):\n",
        "  return VGG(compress_rate = compress_rate)\n"
      ],
      "metadata": {
        "id": "IUy_H5Zq2zEu"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##2. 모델 훈련"
      ],
      "metadata": {
        "id": "EgIGo7WDQHn_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 필요한 라이브러리\n",
        "import os\n",
        "import torchvision\n",
        "from torchvision import datasets\n",
        "from torchvision import transforms\n",
        "from torchvision.transforms import ToTensor\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import MultiStepLR"
      ],
      "metadata": {
        "id": "SONxyY93adgg"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 하이퍼파라미터\n",
        "LEARNING_RATE = 0.01\n",
        "BATCH_SIZE = 128\n",
        "WEIGHT_DECAY = 0.0005\n",
        "MOMENTUM = 0.9\n",
        "NUM_EPOCHS = 250"
      ],
      "metadata": {
        "id": "gocXIYZOQNIZ"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델 훈련 실행 스크립트\n",
        "#-----------------------------------------------------------------------\n",
        "print(\"=============== 1. 설정 시작 ===============\")\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "#-----------------------------------------------------------------------\n",
        "\n",
        "print(\"=============== 2. 데이터 로딩 ===============\")\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.RandomHorizontalFlip(p=0.5),\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.1, hue=0.1),\n",
        "    transforms.RandomRotation(15),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5071, 0.4867, 0.4408), (0.2675, 0.2565, 0.2761)),\n",
        "])\n",
        "\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5071, 0.4867, 0.4408), (0.2675, 0.2565, 0.2761)),\n",
        "])\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR100(root='./data', train=True, download=True, transform=transform_train)\n",
        "train_loader = torch.utils.data.DataLoader(trainset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR100(root='./data', train=False, download=True, transform=transform_test)\n",
        "test_loader = torch.utils.data.DataLoader(testset, batch_size=100, shuffle=False, num_workers=2)\n",
        "print(\"데이터 준비 완료!\")\n",
        "\n",
        "#-----------------------------------------------------------------------\n",
        "\n",
        "print(\"=============== 3. 모델,loss,옵티마이저,스케쥴러 정의 ===============\")\n",
        "model = VGG(num_classes=100, init_weights=True, compress_rate=None).to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=LEARNING_RATE, momentum=MOMENTUM,\n",
        "                          weight_decay=WEIGHT_DECAY, nesterov=True)\n",
        "scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer,\n",
        "                                               milestones=[150, 225],\n",
        "                                               gamma=0.1)\n",
        "print(\"모델 정의 완료!\")\n",
        "\n",
        "#-----------------------------------------------------------------------\n",
        "\n",
        "print(\"=============== 4. 훈련 시작 ===============\")\n",
        "\n",
        "# Early stopping\n",
        "best_accuracy = 0.0\n",
        "patience = 20\n",
        "patience_counter = 0\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    #훈련\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for i, (inputs, labels) in enumerate(train_loader):\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    scheduler.step()\n",
        "\n",
        "    #평가\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in test_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    accuracy = 100 * correct / total\n",
        "\n",
        "    # Early stopping 체크\n",
        "    if accuracy > best_accuracy:\n",
        "        best_accuracy = accuracy\n",
        "        patience_counter = 0\n",
        "        best_model_path = os.path.join(SAVE_DIR, 'best_vgg16_bn_cifar100.pt')\n",
        "        torch.save(model.state_dict(), best_model_path)\n",
        "        print(f\"새로운 최고 성능 모델 저장: {best_model_path}\")\n",
        "    else:\n",
        "        patience_counter += 1\n",
        "\n",
        "    # 로그 출력\n",
        "    print(f'Epoch [{epoch+1}/{NUM_EPOCHS}] | '\n",
        "          f'Loss: {running_loss / len(train_loader):.4f} | '\n",
        "          f'Test Accuracy: {accuracy:.2f} % | '\n",
        "          f'Best Accuracy: {best_accuracy:.2f} % | '\n",
        "          f'Patience: {patience_counter}/{patience} | '\n",
        "          f'Current LR: {optimizer.param_groups[0][\"lr\"]}')\n",
        "\n",
        "    # Early stopping 조건\n",
        "    if patience_counter >= patience:\n",
        "        print(f'\\n=============== Early Stopping ===============')\n",
        "        print(f'훈련이 {epoch+1} 에폭에서 조기 중단되었습니다.')\n",
        "        print(f'최고 정확도: {best_accuracy:.2f}%')\n",
        "        break\n",
        "print('=============== 훈련 종료 ===============')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L06X4RuPVjq_",
        "outputId": "add5a174-78a2-4ab9-c9ce-0c038ed866a4"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=============== 1. 설정 시작 ===============\n",
            "Using device: cuda\n",
            "=============== 2. 데이터 로딩 ===============\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 169M/169M [00:12<00:00, 13.2MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "데이터 준비 완료!\n",
            "=============== 3. 모델,loss,옵티마이저,스케쥴러 정의 ===============\n",
            "모델 정의 완료!\n",
            "=============== 4. 훈련 시작 ===============\n",
            "새로운 최고 성능 모델 저장: ./best_vgg16_bn_cifar100.pt\n",
            "Epoch [1/250] | Loss: 4.0471 | Test Accuracy: 16.37 % | Best Accuracy: 16.37 % | Patience: 0/20 | Current LR: 0.01\n",
            "새로운 최고 성능 모델 저장: ./best_vgg16_bn_cifar100.pt\n",
            "Epoch [2/250] | Loss: 3.6403 | Test Accuracy: 24.12 % | Best Accuracy: 24.12 % | Patience: 0/20 | Current LR: 0.01\n",
            "새로운 최고 성능 모델 저장: ./best_vgg16_bn_cifar100.pt\n",
            "Epoch [3/250] | Loss: 3.3793 | Test Accuracy: 30.89 % | Best Accuracy: 30.89 % | Patience: 0/20 | Current LR: 0.01\n",
            "새로운 최고 성능 모델 저장: ./best_vgg16_bn_cifar100.pt\n",
            "Epoch [4/250] | Loss: 3.1710 | Test Accuracy: 35.36 % | Best Accuracy: 35.36 % | Patience: 0/20 | Current LR: 0.01\n",
            "새로운 최고 성능 모델 저장: ./best_vgg16_bn_cifar100.pt\n",
            "Epoch [5/250] | Loss: 2.9915 | Test Accuracy: 39.13 % | Best Accuracy: 39.13 % | Patience: 0/20 | Current LR: 0.01\n",
            "Epoch [6/250] | Loss: 2.8516 | Test Accuracy: 36.97 % | Best Accuracy: 39.13 % | Patience: 1/20 | Current LR: 0.01\n",
            "새로운 최고 성능 모델 저장: ./best_vgg16_bn_cifar100.pt\n",
            "Epoch [7/250] | Loss: 2.7372 | Test Accuracy: 41.37 % | Best Accuracy: 41.37 % | Patience: 0/20 | Current LR: 0.01\n",
            "새로운 최고 성능 모델 저장: ./best_vgg16_bn_cifar100.pt\n",
            "Epoch [8/250] | Loss: 2.6347 | Test Accuracy: 44.66 % | Best Accuracy: 44.66 % | Patience: 0/20 | Current LR: 0.01\n",
            "새로운 최고 성능 모델 저장: ./best_vgg16_bn_cifar100.pt\n",
            "Epoch [9/250] | Loss: 2.5444 | Test Accuracy: 47.44 % | Best Accuracy: 47.44 % | Patience: 0/20 | Current LR: 0.01\n",
            "새로운 최고 성능 모델 저장: ./best_vgg16_bn_cifar100.pt\n",
            "Epoch [10/250] | Loss: 2.4759 | Test Accuracy: 49.09 % | Best Accuracy: 49.09 % | Patience: 0/20 | Current LR: 0.01\n",
            "새로운 최고 성능 모델 저장: ./best_vgg16_bn_cifar100.pt\n",
            "Epoch [11/250] | Loss: 2.4054 | Test Accuracy: 51.37 % | Best Accuracy: 51.37 % | Patience: 0/20 | Current LR: 0.01\n",
            "새로운 최고 성능 모델 저장: ./best_vgg16_bn_cifar100.pt\n",
            "Epoch [12/250] | Loss: 2.3462 | Test Accuracy: 53.60 % | Best Accuracy: 53.60 % | Patience: 0/20 | Current LR: 0.01\n",
            "Epoch [13/250] | Loss: 2.2963 | Test Accuracy: 52.61 % | Best Accuracy: 53.60 % | Patience: 1/20 | Current LR: 0.01\n",
            "새로운 최고 성능 모델 저장: ./best_vgg16_bn_cifar100.pt\n",
            "Epoch [14/250] | Loss: 2.2448 | Test Accuracy: 54.83 % | Best Accuracy: 54.83 % | Patience: 0/20 | Current LR: 0.01\n",
            "Epoch [15/250] | Loss: 2.1939 | Test Accuracy: 54.63 % | Best Accuracy: 54.83 % | Patience: 1/20 | Current LR: 0.01\n",
            "새로운 최고 성능 모델 저장: ./best_vgg16_bn_cifar100.pt\n",
            "Epoch [16/250] | Loss: 2.1540 | Test Accuracy: 55.65 % | Best Accuracy: 55.65 % | Patience: 0/20 | Current LR: 0.01\n",
            "새로운 최고 성능 모델 저장: ./best_vgg16_bn_cifar100.pt\n",
            "Epoch [17/250] | Loss: 2.1140 | Test Accuracy: 56.54 % | Best Accuracy: 56.54 % | Patience: 0/20 | Current LR: 0.01\n",
            "새로운 최고 성능 모델 저장: ./best_vgg16_bn_cifar100.pt\n",
            "Epoch [18/250] | Loss: 2.0696 | Test Accuracy: 57.34 % | Best Accuracy: 57.34 % | Patience: 0/20 | Current LR: 0.01\n",
            "새로운 최고 성능 모델 저장: ./best_vgg16_bn_cifar100.pt\n",
            "Epoch [19/250] | Loss: 2.0299 | Test Accuracy: 58.24 % | Best Accuracy: 58.24 % | Patience: 0/20 | Current LR: 0.01\n",
            "Epoch [20/250] | Loss: 1.9923 | Test Accuracy: 58.19 % | Best Accuracy: 58.24 % | Patience: 1/20 | Current LR: 0.01\n",
            "새로운 최고 성능 모델 저장: ./best_vgg16_bn_cifar100.pt\n",
            "Epoch [21/250] | Loss: 1.9640 | Test Accuracy: 58.48 % | Best Accuracy: 58.48 % | Patience: 0/20 | Current LR: 0.01\n",
            "새로운 최고 성능 모델 저장: ./best_vgg16_bn_cifar100.pt\n",
            "Epoch [22/250] | Loss: 1.9381 | Test Accuracy: 59.59 % | Best Accuracy: 59.59 % | Patience: 0/20 | Current LR: 0.01\n",
            "Epoch [23/250] | Loss: 1.9036 | Test Accuracy: 59.57 % | Best Accuracy: 59.59 % | Patience: 1/20 | Current LR: 0.01\n",
            "새로운 최고 성능 모델 저장: ./best_vgg16_bn_cifar100.pt\n",
            "Epoch [24/250] | Loss: 1.8747 | Test Accuracy: 60.56 % | Best Accuracy: 60.56 % | Patience: 0/20 | Current LR: 0.01\n",
            "새로운 최고 성능 모델 저장: ./best_vgg16_bn_cifar100.pt\n",
            "Epoch [25/250] | Loss: 1.8430 | Test Accuracy: 61.41 % | Best Accuracy: 61.41 % | Patience: 0/20 | Current LR: 0.01\n",
            "Epoch [26/250] | Loss: 1.8205 | Test Accuracy: 60.89 % | Best Accuracy: 61.41 % | Patience: 1/20 | Current LR: 0.01\n",
            "새로운 최고 성능 모델 저장: ./best_vgg16_bn_cifar100.pt\n",
            "Epoch [27/250] | Loss: 1.7982 | Test Accuracy: 61.47 % | Best Accuracy: 61.47 % | Patience: 0/20 | Current LR: 0.01\n",
            "Epoch [28/250] | Loss: 1.7667 | Test Accuracy: 61.38 % | Best Accuracy: 61.47 % | Patience: 1/20 | Current LR: 0.01\n",
            "Epoch [29/250] | Loss: 1.7429 | Test Accuracy: 61.09 % | Best Accuracy: 61.47 % | Patience: 2/20 | Current LR: 0.01\n",
            "Epoch [30/250] | Loss: 1.7313 | Test Accuracy: 61.10 % | Best Accuracy: 61.47 % | Patience: 3/20 | Current LR: 0.01\n",
            "새로운 최고 성능 모델 저장: ./best_vgg16_bn_cifar100.pt\n",
            "Epoch [31/250] | Loss: 1.7009 | Test Accuracy: 62.08 % | Best Accuracy: 62.08 % | Patience: 0/20 | Current LR: 0.01\n",
            "Epoch [32/250] | Loss: 1.6837 | Test Accuracy: 61.09 % | Best Accuracy: 62.08 % | Patience: 1/20 | Current LR: 0.01\n",
            "새로운 최고 성능 모델 저장: ./best_vgg16_bn_cifar100.pt\n",
            "Epoch [33/250] | Loss: 1.6586 | Test Accuracy: 63.37 % | Best Accuracy: 63.37 % | Patience: 0/20 | Current LR: 0.01\n",
            "Epoch [34/250] | Loss: 1.6381 | Test Accuracy: 62.09 % | Best Accuracy: 63.37 % | Patience: 1/20 | Current LR: 0.01\n",
            "Epoch [35/250] | Loss: 1.6221 | Test Accuracy: 61.64 % | Best Accuracy: 63.37 % | Patience: 2/20 | Current LR: 0.01\n",
            "Epoch [36/250] | Loss: 1.6019 | Test Accuracy: 60.88 % | Best Accuracy: 63.37 % | Patience: 3/20 | Current LR: 0.01\n",
            "Epoch [37/250] | Loss: 1.5863 | Test Accuracy: 63.13 % | Best Accuracy: 63.37 % | Patience: 4/20 | Current LR: 0.01\n",
            "Epoch [38/250] | Loss: 1.5720 | Test Accuracy: 62.29 % | Best Accuracy: 63.37 % | Patience: 5/20 | Current LR: 0.01\n",
            "Epoch [39/250] | Loss: 1.5541 | Test Accuracy: 62.43 % | Best Accuracy: 63.37 % | Patience: 6/20 | Current LR: 0.01\n",
            "Epoch [40/250] | Loss: 1.5391 | Test Accuracy: 62.77 % | Best Accuracy: 63.37 % | Patience: 7/20 | Current LR: 0.01\n",
            "Epoch [41/250] | Loss: 1.5194 | Test Accuracy: 62.52 % | Best Accuracy: 63.37 % | Patience: 8/20 | Current LR: 0.01\n",
            "Epoch [42/250] | Loss: 1.5023 | Test Accuracy: 63.23 % | Best Accuracy: 63.37 % | Patience: 9/20 | Current LR: 0.01\n",
            "Epoch [43/250] | Loss: 1.4817 | Test Accuracy: 62.30 % | Best Accuracy: 63.37 % | Patience: 10/20 | Current LR: 0.01\n",
            "Epoch [44/250] | Loss: 1.4685 | Test Accuracy: 62.63 % | Best Accuracy: 63.37 % | Patience: 11/20 | Current LR: 0.01\n",
            "Epoch [45/250] | Loss: 1.4639 | Test Accuracy: 63.19 % | Best Accuracy: 63.37 % | Patience: 12/20 | Current LR: 0.01\n",
            "Epoch [46/250] | Loss: 1.4428 | Test Accuracy: 62.33 % | Best Accuracy: 63.37 % | Patience: 13/20 | Current LR: 0.01\n",
            "Epoch [47/250] | Loss: 1.4463 | Test Accuracy: 62.12 % | Best Accuracy: 63.37 % | Patience: 14/20 | Current LR: 0.01\n",
            "Epoch [48/250] | Loss: 1.4261 | Test Accuracy: 61.35 % | Best Accuracy: 63.37 % | Patience: 15/20 | Current LR: 0.01\n",
            "Epoch [49/250] | Loss: 1.4150 | Test Accuracy: 61.80 % | Best Accuracy: 63.37 % | Patience: 16/20 | Current LR: 0.01\n",
            "Epoch [50/250] | Loss: 1.4059 | Test Accuracy: 62.19 % | Best Accuracy: 63.37 % | Patience: 17/20 | Current LR: 0.01\n",
            "Epoch [51/250] | Loss: 1.3913 | Test Accuracy: 62.35 % | Best Accuracy: 63.37 % | Patience: 18/20 | Current LR: 0.01\n",
            "Epoch [52/250] | Loss: 1.3743 | Test Accuracy: 61.95 % | Best Accuracy: 63.37 % | Patience: 19/20 | Current LR: 0.01\n",
            "Epoch [53/250] | Loss: 1.3661 | Test Accuracy: 61.70 % | Best Accuracy: 63.37 % | Patience: 20/20 | Current LR: 0.01\n",
            "\n",
            "=============== Early Stopping ===============\n",
            "훈련이 53 에폭에서 조기 중단되었습니다.\n",
            "최고 정확도: 63.37%\n",
            "=============== 훈련 종료 ===============\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.랭크 계산 함수"
      ],
      "metadata": {
        "id": "3wACv4-OP410"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 필요한 라이브러리\n",
        "from collections import defaultdict\n",
        "import torchvision.transforms as transforms\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "ZYlnOWflQNrb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class HRankFeatureMapCalculator:\n",
        "    def __init__(self, model, device):\n",
        "        self.model = model\n",
        "        self.device = device\n",
        "        self.feature_result = torch.tensor(0.)#각 레이어의 채널별 랭크 합계 누적할 텐서\n",
        "        self.total = torch.tensor(0.)#랭크 계산에 사용된 총 데이터 샘플 수를 누적할 텐서\n",
        "        self.current_ranks = {} # 현재 계산된 랭크를 저장하기 위한 딕셔너리\n",
        "\n",
        "    def get_feature_hook(self, layer_name):\n",
        "        def hook_fn(module, input, output):\n",
        "            #output 텐서의 모양은 [batch,ch,H,W]\n",
        "            a = output.shape[0]  # batch_size\n",
        "            b = output.shape[1]  # num_channels\n",
        "            # 각 샘플, 각 채널에 대해 matrix rank 계산\n",
        "            c = torch.tensor([\n",
        "                torch.linalg.matrix_rank(output[i, j, :, :]).item()\n",
        "                for i in range(a)\n",
        "                for j in range(b)\n",
        "            ])\n",
        "\n",
        "            # reshape하여 [batch_size, num_channels] 형태로 만들고 합계 계산\n",
        "            c = c.view(a, -1).float()\n",
        "            c = c.sum(0)  # 배치 차원에서 합계 (각 채널별 총 rank)\n",
        "\n",
        "            # 누적 평균 계산\n",
        "            self.feature_result = self.feature_result * self.total + c\n",
        "            self.total = self.total + a\n",
        "            self.feature_result = self.feature_result / self.total\n",
        "\n",
        "        return hook_fn\n",
        "\n",
        "    #all_ranks 딕셔너리를 만들기 위한 함수.\n",
        "    #키값은 'conv%d'이고 value는  array([rank_ch0, rank_ch1, rank_ch2, ..., rank_ch63])로 된 딕셔너리를 반환함.\n",
        "    def calculate_ranks_hrank_style(self, dataloader, limit=5):\n",
        "        self.model.eval()\n",
        "        all_ranks = {}\n",
        "\n",
        "        # 각 ReLU 레이어에 대해 순차적으로 랭크 계산\n",
        "        for layer_idx, relu_idx in enumerate(relucfg):\n",
        "            layer_name = f\"conv{layer_idx + 1}\"\n",
        "            relu_module = self.model.features[relu_idx]  # features의 relu_idx 위치\n",
        "\n",
        "            print(f\"\\n{layer_name} 랭크 계산 중...\")\n",
        "\n",
        "            # 초기화\n",
        "            self.feature_result = torch.tensor(0.)\n",
        "            self.total = torch.tensor(0.)\n",
        "\n",
        "            # Hook 등록\n",
        "            hook = relu_module.register_forward_hook(self.get_feature_hook(layer_name))\n",
        "\n",
        "            try:\n",
        "                with torch.no_grad():\n",
        "                    for batch_idx, (inputs, targets) in enumerate(dataloader):\n",
        "                        if batch_idx >= limit:\n",
        "                            break\n",
        "\n",
        "                        inputs = inputs.to(self.device)\n",
        "                        _ = self.model(inputs)\n",
        "\n",
        "                # 결과 저장\n",
        "                all_ranks[layer_name] = self.feature_result.clone().numpy()\n",
        "                print(f\"  완료 - 채널 수: {len(all_ranks[layer_name])}\")\n",
        "                print(f\"  평균 랭크: {all_ranks[layer_name].mean():.2f}, 범위: {all_ranks[layer_name].min():.0f}~{all_ranks[layer_name].max():.0f}\")\n",
        "\n",
        "            finally:\n",
        "                hook.remove()\n",
        "\n",
        "        return all_ranks\n",
        "\n",
        "    def get_layer_info(self, dataloader):\n",
        "        \"\"\"각 레이어의 정보 수집 (채널 수, feature map 크기 등)\"\"\"\n",
        "        layer_info = {}\n",
        "        hooks = []\n",
        "        layer_outputs = {}\n",
        "\n",
        "        def make_info_hook(layer_name):\n",
        "            def hook_fn(module, input, output):\n",
        "                layer_outputs[layer_name] = {\n",
        "                    'shape': output.shape,\n",
        "                    'num_channels': output.shape[1],\n",
        "                    'height': output.shape[2],\n",
        "                    'width': output.shape[3],\n",
        "                    'max_possible_rank': min(output.shape[2], output.shape[3])\n",
        "                }\n",
        "            return hook_fn\n",
        "\n",
        "        # 모든 ReLU 레이어에 hook 등록\n",
        "        for layer_idx, relu_idx in enumerate(relucfg):\n",
        "            layer_name = f\"conv{layer_idx + 1}\"\n",
        "            relu_module = self.model.features[relu_idx]  # features의 relu_idx 위치\n",
        "            hook = relu_module.register_forward_hook(make_info_hook(layer_name))\n",
        "            hooks.append(hook)\n",
        "\n",
        "        # 한 번 forward pass\n",
        "        self.model.eval()\n",
        "        with torch.no_grad():\n",
        "            inputs, _ = next(iter(dataloader))\n",
        "            inputs = inputs.to(self.device)\n",
        "            _ = self.model(inputs)\n",
        "\n",
        "        # Hook 제거\n",
        "        for hook in hooks:\n",
        "            hook.remove()\n",
        "\n",
        "        return layer_outputs\n",
        "\n",
        "  #---------------------여기부터는 보조 함수------------------------------------\n",
        "    def visualize_hrank_results(self, all_ranks, layer_info, save_path=None):\n",
        "        \"\"\"HRank 결과 시각화\"\"\"\n",
        "        num_layers = len(all_ranks)\n",
        "\n",
        "        # 서브플롯 배치 계산\n",
        "        if num_layers <= 4:\n",
        "            rows, cols = 2, 2\n",
        "        elif num_layers <= 6:\n",
        "            rows, cols = 2, 3\n",
        "        elif num_layers <= 9:\n",
        "            rows, cols = 3, 3\n",
        "        else:\n",
        "            rows, cols = 4, (num_layers + 3) // 4\n",
        "\n",
        "        fig, axes = plt.subplots(rows, cols, figsize=(15, 10))\n",
        "        axes = axes.flatten() if num_layers > 1 else [axes]\n",
        "\n",
        "        for idx, (layer_name, ranks) in enumerate(all_ranks.items()):\n",
        "            if idx >= len(axes):\n",
        "                break\n",
        "\n",
        "            ax = axes[idx]\n",
        "\n",
        "            # 히스토그램\n",
        "            ax.hist(ranks, bins=min(20, len(np.unique(ranks))), alpha=0.7,\n",
        "                   edgecolor='black', color='skyblue')\n",
        "\n",
        "            # 제목과 레이블\n",
        "            info = layer_info.get(layer_name, {})\n",
        "            num_channels = info.get('num_channels', len(ranks))\n",
        "            fm_size = f\"{info.get('height', '?')}x{info.get('width', '?')}\"\n",
        "\n",
        "            ax.set_title(f'{layer_name}\\n({num_channels} channels, {fm_size})')\n",
        "            ax.set_xlabel('Rank')\n",
        "            ax.set_ylabel('Number of Channels')\n",
        "            ax.grid(True, alpha=0.3)\n",
        "\n",
        "            # 통계 정보\n",
        "            ax.axvline(ranks.mean(), color='red', linestyle='--',\n",
        "                      label=f'Mean: {ranks.mean():.1f}')\n",
        "            ax.legend()\n",
        "\n",
        "            # 텍스트 박스에 상세 정보\n",
        "            textstr = f'Min: {ranks.min():.0f}\\nMax: {ranks.max():.0f}\\nStd: {ranks.std():.1f}'\n",
        "            props = dict(boxstyle='round', facecolor='wheat', alpha=0.5)\n",
        "            ax.text(0.95, 0.95, textstr, transform=ax.transAxes, fontsize=8,\n",
        "                   verticalalignment='top', bbox=props, ha='right')\n",
        "\n",
        "        # 빈 subplot 숨기기\n",
        "        for idx in range(len(all_ranks), len(axes)):\n",
        "            axes[idx].set_visible(False)\n",
        "\n",
        "        plt.tight_layout()\n",
        "\n",
        "        plt.show()\n",
        "\n",
        "    def print_hrank_summary(self, all_ranks, layer_info):\n",
        "        \"\"\"HRank 결과 요약 출력\"\"\"\n",
        "        print(\"\\n\" + \"=\"*70)\n",
        "        print(\"HRank Feature Map 랭크 계산 결과 요약\")\n",
        "        print(\"=\"*70)\n",
        "\n",
        "        total_channels = 0\n",
        "        total_low_rank = 0\n",
        "\n",
        "        for layer_name, ranks in all_ranks.items():\n",
        "            info = layer_info.get(layer_name, {})\n",
        "\n",
        "            print(f\"\\n{layer_name}:\")\n",
        "            print(f\"  채널 수: {len(ranks)}\")\n",
        "            if 'height' in info and 'width' in info:\n",
        "                print(f\"  Feature Map 크기: {info['height']}x{info['width']}\")\n",
        "                print(f\"  최대 가능 랭크: {info['max_possible_rank']}\")\n",
        "\n",
        "            print(f\"  평균 랭크: {ranks.mean():.2f}\")\n",
        "            print(f\"  랭크 범위: {ranks.min():.0f} ~ {ranks.max():.0f}\")\n",
        "            print(f\"  랭크 표준편차: {ranks.std():.2f}\")\n",
        "\n",
        "            # 낮은 랭크 비율 계산\n",
        "            if 'max_possible_rank' in info:\n",
        "                low_rank_threshold = info['max_possible_rank'] * 0.5\n",
        "                low_rank_count = (ranks < low_rank_threshold).sum()\n",
        "                low_rank_ratio = (low_rank_count / len(ranks)) * 100\n",
        "                print(f\"  낮은 랭크 채널 (< {low_rank_threshold:.1f}): {low_rank_count}/{len(ranks)} ({low_rank_ratio:.1f}%)\")\n",
        "\n",
        "                total_channels += len(ranks)\n",
        "                total_low_rank += low_rank_count\n",
        "\n",
        "        if total_channels > 0:\n",
        "            overall_low_rank_ratio = (total_low_rank / total_channels) * 100\n",
        "            print(f\"\\n전체 요약:\")\n",
        "            print(f\"  총 채널 수: {total_channels}\")\n",
        "            print(f\"  낮은 랭크 채널: {total_low_rank} ({overall_low_rank_ratio:.1f}%)\")\n",
        "\n",
        "def test_hrank_implementation(model, test_loader, device, limit=5):\n",
        "    \"\"\"HRank 구현 테스트\"\"\"\n",
        "    print(\"=\"*60)\n",
        "    print(\" 랭크 계산 테스트\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    # 계산기 초기화\n",
        "    calculator = HRankFeatureMapCalculator(model, device)\n",
        "\n",
        "    # 레이어 정보 수집\n",
        "    print(\"\\n1. 레이어 정보 수집 중...\")\n",
        "    layer_info = calculator.get_layer_info(test_loader)\n",
        "\n",
        "    # 랭크 계산\n",
        "    print(\"\\n2. 랭크 계산 중...\")\n",
        "    all_ranks = calculator.calculate_ranks_hrank_style(test_loader, limit=limit)\n",
        "\n",
        "    # 결과 요약\n",
        "    calculator.print_hrank_summary(all_ranks, layer_info)\n",
        "\n",
        "    # 시각화\n",
        "    print(\"\\n3. 결과 시각화...\")\n",
        "    calculator.visualize_hrank_results(all_ranks, layer_info, save_path='hrank_results.png')\n",
        "\n",
        "    return all_ranks, layer_info\n"
      ],
      "metadata": {
        "id": "iryc8114QBgO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 실행"
      ],
      "metadata": {
        "id": "G1_in7ci4lnL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 랭크 계산 실행 스크립트\n",
        "\n",
        "# 디바이스 설정\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"사용 디바이스: {device}\")\n",
        "\n",
        "# 모델 로드\n",
        "model = VGG(num_classes=100, init_weights=True, compress_rate=None).to(device)\n",
        "\n",
        "# 체크포인트 로드\n",
        "checkpoint_path = './best_vgg16_bn_cifar100.pt'\n",
        "model.load_state_dict(torch.load(checkpoint_path, map_location=device))\n",
        "print(\"기존 모델 가중치 로드 완료\")\n",
        "\n",
        "# CIFAR-100 데이터로더\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5071, 0.4867, 0.4408), (0.2675, 0.2565, 0.2761))])\n",
        "\n",
        "testset = torchvision.datasets.CIFAR100(\n",
        "    root='./data', train=False, download=True, transform=transform_test)\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    testset, batch_size=100, shuffle=False, num_workers=2)\n",
        "\n",
        "print(\"CIFAR-100 테스트 데이터 로드 완료\")\n",
        "\n",
        "# 모델이 제대로 로드되었는지 간단한 테스트\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    sample_batch = next(iter(test_loader))\n",
        "    sample_output = model(sample_batch[0][:1].to(device))\n",
        "    print(f\"모델 출력 shape: {sample_output.shape}\")\n",
        "    print(f\"예측 클래스: {torch.argmax(sample_output, dim=1).item()}\")\n",
        "\n",
        "# HRank 테스트 실행\n",
        "all_ranks, layer_info = test_hrank_implementation(model, test_loader, device, limit=5)"
      ],
      "metadata": {
        "id": "zc5Udi4zb2AR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##4. 필터선택, 프루닝 계획 함수"
      ],
      "metadata": {
        "id": "fwAHgsVBb_ME"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 필요한 라이브러리\n",
        "from typing import Dict, List, Tuple"
      ],
      "metadata": {
        "id": "XzBvxSG5cB6u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class HRankPruningPlanner:\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def determine_pruning_plan(self, all_ranks: Dict, pruning_ratios: List[float]):\n",
        "        \"\"\"\n",
        "        HRank 기반 프루닝 계획 수립\n",
        "\n",
        "        Args:\n",
        "            all_ranks: 각 레이어별 랭크 정보 {'conv1': array, 'conv2': array, ...}\n",
        "            pruning_ratios: 각 레이어별 프루닝 비율 [0.3, 0.5, 0.2, ...]\n",
        "\n",
        "        Returns:\n",
        "            pruning_plan: 각 레이어별 프루닝 계획\n",
        "            new_channels: 프루닝 후 각 레이어의 채널 수\n",
        "        \"\"\"\n",
        "        pruning_plan = {}\n",
        "        new_channels = {}\n",
        "\n",
        "        layer_names = list(all_ranks.keys())\n",
        "\n",
        "        # 프루닝 비율 검증 및 조정\n",
        "        if len(pruning_ratios) == 1:\n",
        "            # 모든 레이어에 동일한 비율 적용\n",
        "            pruning_ratios = pruning_ratios * len(layer_names)\n",
        "            print(f\"모든 레이어에 {pruning_ratios[0]*100:.1f}% 프루닝 비율 적용\")\n",
        "        elif len(pruning_ratios) != len(layer_names):\n",
        "            raise ValueError(f\"프루닝 비율 개수({len(pruning_ratios)})와 레이어 수({len(layer_names)})가 맞지 않습니다.\")\n",
        "\n",
        "        print(\"\\n\" + \"=\"*70)\n",
        "        print(\"HRank 기반 프루닝 계획 수립\")\n",
        "        print(\"=\"*70)\n",
        "\n",
        "        total_original_channels = 0\n",
        "        total_pruned_channels = 0\n",
        "\n",
        "        for i, (layer_name, ranks) in enumerate(all_ranks.items()):\n",
        "            ratio = pruning_ratios[i]\n",
        "            original_channels = len(ranks)\n",
        "            total_original_channels += original_channels\n",
        "\n",
        "            if ratio > 0 and ratio < 1.0:\n",
        "                # 낮은 랭크 순으로 정렬하여 제거할 인덱스 선택\n",
        "                num_to_prune = int(original_channels * ratio)\n",
        "                # 최소 1개 채널은 유지\n",
        "                num_to_prune = min(num_to_prune, original_channels - 1)\n",
        "\n",
        "                # 랭크 기준으로 정렬 (낮은 랭크부터)\n",
        "                sorted_indices = np.argsort(ranks)\n",
        "                prune_indices = sorted_indices[:num_to_prune]\n",
        "                keep_indices = sorted_indices[num_to_prune:]\n",
        "\n",
        "                # 프루닝 계획 저장\n",
        "                pruning_plan[layer_name] = {\n",
        "                    'prune_indices': prune_indices.tolist(),\n",
        "                    'keep_indices': keep_indices.tolist(),\n",
        "                    'original_channels': original_channels,\n",
        "                    'pruned_channels': len(keep_indices),\n",
        "                    'pruning_ratio_actual': len(prune_indices) / original_channels,\n",
        "                    'pruning_ratio_target': ratio,\n",
        "                    'pruned_ranks': ranks[prune_indices],  # 제거될 채널들의 랭크\n",
        "                    'kept_ranks': ranks[keep_indices]      # 유지될 채널들의 랭크\n",
        "                }\n",
        "\n",
        "                new_channels[layer_name] = len(keep_indices)\n",
        "                total_pruned_channels += len(keep_indices)\n",
        "\n",
        "            elif ratio >= 1.0:\n",
        "                print(f\"경고: {layer_name}의 프루닝 비율이 1.0 이상입니다. 프루닝하지 않습니다.\")\n",
        "                keep_indices = list(range(original_channels))\n",
        "                pruning_plan[layer_name] = {\n",
        "                    'prune_indices': [],\n",
        "                    'keep_indices': keep_indices,\n",
        "                    'original_channels': original_channels,\n",
        "                    'pruned_channels': original_channels,\n",
        "                    'pruning_ratio_actual': 0.0,\n",
        "                    'pruning_ratio_target': ratio,\n",
        "                    'pruned_ranks': np.array([]),\n",
        "                    'kept_ranks': ranks\n",
        "                }\n",
        "                new_channels[layer_name] = original_channels\n",
        "                total_pruned_channels += original_channels\n",
        "\n",
        "            else:\n",
        "                # 프루닝하지 않음 (ratio == 0)\n",
        "                keep_indices = list(range(original_channels))\n",
        "                pruning_plan[layer_name] = {\n",
        "                    'prune_indices': [],\n",
        "                    'keep_indices': keep_indices,\n",
        "                    'original_channels': original_channels,\n",
        "                    'pruned_channels': original_channels,\n",
        "                    'pruning_ratio_actual': 0.0,\n",
        "                    'pruning_ratio_target': ratio,\n",
        "                    'pruned_ranks': np.array([]),\n",
        "                    'kept_ranks': ranks\n",
        "                }\n",
        "                new_channels[layer_name] = original_channels\n",
        "                total_pruned_channels += original_channels\n",
        "\n",
        "            # 결과 출력\n",
        "            plan = pruning_plan[layer_name]\n",
        "            print(f\"{layer_name:>6}: {plan['original_channels']:>3} -> {plan['pruned_channels']:>3} 채널 \"\n",
        "                  f\"({plan['pruning_ratio_actual']*100:>5.1f}% 프루닝)\")\n",
        "\n",
        "            if len(plan['prune_indices']) > 0:\n",
        "                print(f\"        제거될 채널 랭크 범위: {plan['pruned_ranks'].min():.1f} ~ {plan['pruned_ranks'].max():.1f}\")\n",
        "                print(f\"        유지될 채널 랭크 범위: {plan['kept_ranks'].min():.1f} ~ {plan['kept_ranks'].max():.1f}\")\n",
        "\n",
        "        # 전체 요약\n",
        "        overall_pruning_ratio = 1 - (total_pruned_channels / total_original_channels)\n",
        "        print(f\"\\n전체 요약:\")\n",
        "        print(f\"  총 원본 채널 수: {total_original_channels}\")\n",
        "        print(f\"  총 프루닝 후 채널 수: {total_pruned_channels}\")\n",
        "        print(f\"  전체 프루닝 비율: {overall_pruning_ratio*100:.1f}%\")\n",
        "\n",
        "        return pruning_plan, new_channels\n",
        "\n",
        "    def determine_classifier_pruning(self, classifier_state_dict: Dict, pruning_ratio: float):\n",
        "        \"\"\"\n",
        "        2-layer Classifier 프루닝 계획 수립\n",
        "\n",
        "        Args:\n",
        "            classifier_state_dict: classifier의 state_dict (torch.load로 로드한 .pt 파일 또는 model.classifier.state_dict())\n",
        "            pruning_ratio: 중간 hidden layer 프루닝 비율 (0.0 ~ 1.0)\n",
        "\n",
        "        Returns:\n",
        "            classifier_plan: Classifier 프루닝 계획\n",
        "\n",
        "        사용 예시:\n",
        "            # 방법 1: .pt 파일에서 직접 로드\n",
        "            classifier_weights = torch.load('classifier.pt')\n",
        "            plan = planner.determine_classifier_pruning(classifier_weights, 0.3)\n",
        "        \"\"\"\n",
        "        print(\"\\n\" + \"=\"*70)\n",
        "        print(\"2-Layer Classifier 프루닝 계획 수립\")\n",
        "        print(\"=\"*70)\n",
        "\n",
        "        # state_dict에서 가중치 추출\n",
        "        if 'linear1.weight' in classifier_state_dict:\n",
        "            linear1_weight = classifier_state_dict['linear1.weight'].cpu().numpy()\n",
        "            linear2_weight = classifier_state_dict['linear2.weight'].cpu().numpy()\n",
        "        else:\n",
        "            raise KeyError(\"classifier_state_dict에 'linear1.weight' 또는 'linear2.weight' 키가 없습니다. \"\n",
        "                          \"키 목록: \" + str(list(classifier_state_dict.keys())))\n",
        "\n",
        "        # 차원 정보\n",
        "        hidden_dim, in_features = linear1_weight.shape\n",
        "        num_classes, hidden_dim_check = linear2_weight.shape\n",
        "\n",
        "        if hidden_dim != hidden_dim_check:\n",
        "            raise ValueError(f\"linear1 출력 차원({hidden_dim})과 linear2 입력 차원({hidden_dim_check})이 일치하지 않습니다.\")\n",
        "\n",
        "        print(f\"구조: {in_features} -> {hidden_dim} -> {num_classes}\")\n",
        "\n",
        "        if pruning_ratio <= 0 or pruning_ratio >= 1.0:\n",
        "            print(f\"경고: 프루닝 비율이 {pruning_ratio}입니다. 프루닝하지 않습니다.\")\n",
        "            classifier_plan = {\n",
        "                'prune_indices': [],\n",
        "                'keep_indices': list(range(hidden_dim)),\n",
        "                'original_hidden_dim': hidden_dim,\n",
        "                'pruned_hidden_dim': hidden_dim,\n",
        "                'pruning_ratio_actual': 0.0,\n",
        "                'pruning_ratio_target': pruning_ratio\n",
        "            }\n",
        "        else:\n",
        "            # 각 hidden 뉴런의 중요도 계산\n",
        "            # linear1의 출력 가중치 (각 행의 L2 norm)\n",
        "            linear1_importance = np.linalg.norm(linear1_weight, axis=1)  # shape: [hidden_dim]\n",
        "\n",
        "            # linear2의 입력 가중치 (각 열의 L2 norm)\n",
        "            linear2_importance = np.linalg.norm(linear2_weight, axis=0)  # shape: [hidden_dim]\n",
        "\n",
        "            # 두 중요도를 결합 (곱셈 또는 합)\n",
        "            # 곱셈: 양쪽 모두에서 중요한 뉴런 선호\n",
        "            combined_importance = linear1_importance * linear2_importance\n",
        "\n",
        "            # 프루닝할 뉴런 수 계산\n",
        "            num_to_prune = int(hidden_dim * pruning_ratio)\n",
        "            num_to_prune = min(num_to_prune, hidden_dim - 1)  # 최소 1개는 유지\n",
        "\n",
        "            # 중요도가 낮은 순으로 정렬\n",
        "            sorted_indices = np.argsort(combined_importance)\n",
        "            prune_indices = sorted_indices[:num_to_prune]\n",
        "            keep_indices = sorted_indices[num_to_prune:]\n",
        "\n",
        "            classifier_plan = {\n",
        "                'prune_indices': prune_indices.tolist(),\n",
        "                'keep_indices': keep_indices.tolist(),\n",
        "                'original_hidden_dim': hidden_dim,\n",
        "                'pruned_hidden_dim': len(keep_indices),\n",
        "                'pruning_ratio_actual': len(prune_indices) / hidden_dim,\n",
        "                'pruning_ratio_target': pruning_ratio,\n",
        "                'pruned_importance': combined_importance[prune_indices],\n",
        "                'kept_importance': combined_importance[keep_indices],\n",
        "                'linear1_importance': linear1_importance,\n",
        "                'linear2_importance': linear2_importance\n",
        "            }\n",
        "\n",
        "        # 결과 출력\n",
        "        print(f\"Hidden Layer: {classifier_plan['original_hidden_dim']:>5} -> {classifier_plan['pruned_hidden_dim']:>5} 뉴런 \"\n",
        "              f\"({classifier_plan['pruning_ratio_actual']*100:>5.1f}% 프루닝)\")\n",
        "        print(f\"  최종 구조: {in_features} -> {classifier_plan['pruned_hidden_dim']} -> {num_classes}\")\n",
        "\n",
        "        if len(classifier_plan['prune_indices']) > 0:\n",
        "            print(f\"  제거될 뉴런 중요도 범위: {classifier_plan['pruned_importance'].min():.4f} ~ \"\n",
        "                  f\"{classifier_plan['pruned_importance'].max():.4f}\")\n",
        "            print(f\"  유지될 뉴런 중요도 범위: {classifier_plan['kept_importance'].min():.4f} ~ \"\n",
        "                  f\"{classifier_plan['kept_importance'].max():.4f}\")\n",
        "\n",
        "        return classifier_plan\n",
        "    #----------------------------보조 함수--------------------------------------------\n",
        "\n",
        "    def visualize_pruning_plan(self, all_ranks: Dict, pruning_plan: Dict, save_path=None):\n",
        "        \"\"\"프루닝 계획 시각화\"\"\"\n",
        "        num_layers = len(all_ranks)\n",
        "\n",
        "        if num_layers <= 4:\n",
        "            rows, cols = 2, 2\n",
        "        elif num_layers <= 6:\n",
        "            rows, cols = 2, 3\n",
        "        else:\n",
        "            rows, cols = 3, (num_layers + 2) // 3\n",
        "\n",
        "        fig, axes = plt.subplots(rows, cols, figsize=(15, 10))\n",
        "        if num_layers == 1:\n",
        "            axes = [axes]\n",
        "        else:\n",
        "            axes = axes.flatten()\n",
        "\n",
        "        for idx, (layer_name, ranks) in enumerate(all_ranks.items()):\n",
        "            if idx >= len(axes):\n",
        "                break\n",
        "\n",
        "            ax = axes[idx]\n",
        "            plan = pruning_plan[layer_name]\n",
        "\n",
        "            # 전체 랭크 히스토그램\n",
        "            ax.hist(ranks, bins=20, alpha=0.3, color='gray', label='All channels', density=True)\n",
        "\n",
        "            # 제거될 채널들의 랭크\n",
        "            if len(plan['pruned_ranks']) > 0:\n",
        "                ax.hist(plan['pruned_ranks'], bins=15, alpha=0.7, color='red',\n",
        "                       label=f'Pruned ({len(plan[\"pruned_ranks\"])})', density=True)\n",
        "\n",
        "            # 유지될 채널들의 랭크\n",
        "            if len(plan['kept_ranks']) > 0:\n",
        "                ax.hist(plan['kept_ranks'], bins=15, alpha=0.7, color='blue',\n",
        "                       label=f'Kept ({len(plan[\"kept_ranks\"])})', density=True)\n",
        "\n",
        "            ax.set_title(f'{layer_name}\\n{plan[\"original_channels\"]} -> {plan[\"pruned_channels\"]} '\n",
        "                        f'({plan[\"pruning_ratio_actual\"]*100:.1f}%)')\n",
        "            ax.set_xlabel('Rank')\n",
        "            ax.set_ylabel('Density')\n",
        "            ax.legend()\n",
        "            ax.grid(True, alpha=0.3)\n",
        "\n",
        "        # 빈 subplot 숨기기\n",
        "        for idx in range(len(all_ranks), len(axes)):\n",
        "            axes[idx].set_visible(False)\n",
        "\n",
        "        plt.tight_layout()\n",
        "\n",
        "        if save_path:\n",
        "            plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
        "            print(f\"프루닝 계획 시각화가 {save_path}에 저장되었습니다.\")\n",
        "\n",
        "        plt.show()\n",
        "\n",
        "    def validate_pruning_plan(self, pruning_plan: Dict):\n",
        "        \"\"\"프루닝 계획 검증\"\"\"\n",
        "        print(\"\\n프루닝 계획 검증 중...\")\n",
        "\n",
        "        validation_passed = True\n",
        "\n",
        "        for layer_name, plan in pruning_plan.items():\n",
        "            # 인덱스 중복 검사\n",
        "            prune_set = set(plan['prune_indices'])\n",
        "            keep_set = set(plan['keep_indices'])\n",
        "\n",
        "            if prune_set & keep_set:  # 교집합이 있으면\n",
        "                print(f\"오류: {layer_name}에서 제거/유지 인덱스가 중복됩니다.\")\n",
        "                validation_passed = False\n",
        "\n",
        "            # 전체 인덱스 검사\n",
        "            all_indices = set(range(plan['original_channels']))\n",
        "            plan_indices = prune_set | keep_set\n",
        "\n",
        "            if all_indices != plan_indices:\n",
        "                print(f\"오류: {layer_name}에서 인덱스가 누락되거나 초과됩니다.\")\n",
        "                validation_passed = False\n",
        "\n",
        "            # 최소 채널 수 검사\n",
        "            if plan['pruned_channels'] < 1:\n",
        "                print(f\"오류: {layer_name}에서 유지되는 채널이 없습니다.\")\n",
        "                validation_passed = False\n",
        "\n",
        "        if validation_passed:\n",
        "            print(\"프루닝 계획 검증 통과!\")\n",
        "        else:\n",
        "            print(\"프루닝 계획에 오류가 있습니다.\")\n",
        "\n",
        "        return validation_passed\n",
        "\n",
        "\n",
        "def test_pruning_planner(all_ranks, pruning_ratios, classifier_weights=None, classifier_ratio=0.0):\n",
        "    \"\"\"\n",
        "    프루닝 계획 수립 테스트 (Conv + Classifier)\n",
        "\n",
        "    Args:\n",
        "        all_ranks: Conv 레이어별 랭크 정보\n",
        "        pruning_ratios: Conv 레이어별 프루닝 비율\n",
        "        classifier_weights: Classifier state_dict (optional, .pt 파일 또는 model.classifier.state_dict())\n",
        "        classifier_ratio: Classifier 프루닝 비율 (optional, 0.0이면 프루닝 안함)\n",
        "\n",
        "    Returns:\n",
        "        pruning_plan: Conv 레이어 프루닝 계획\n",
        "        new_channels: 프루닝 후 채널 수\n",
        "        classifier_plan: Classifier 프루닝 계획 (없으면 None)\n",
        "        is_valid: 계획 검증 결과\n",
        "    \"\"\"\n",
        "    print(\"=\"*60)\n",
        "    print(\"HRank 프루닝 계획 수립 테스트\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    planner = HRankPruningPlanner()\n",
        "\n",
        "    # 1. Conv 레이어 프루닝 계획 수립\n",
        "    pruning_plan, new_channels = planner.determine_pruning_plan(all_ranks, pruning_ratios)\n",
        "\n",
        "    # 2. Classifier 프루닝 계획 수립 (옵션)\n",
        "    classifier_plan = None\n",
        "    if classifier_weights is not None and classifier_ratio > 0:\n",
        "        classifier_plan = planner.determine_classifier_pruning(classifier_weights, classifier_ratio)\n",
        "\n",
        "    # 3. 계획 검증\n",
        "    is_valid = planner.validate_pruning_plan(pruning_plan)\n",
        "\n",
        "    # 4. 시각화\n",
        "    if is_valid:\n",
        "        planner.visualize_pruning_plan(all_ranks, pruning_plan, save_path='pruning_plan.png')\n",
        "\n",
        "    return pruning_plan, new_channels, classifier_plan, is_valid\n"
      ],
      "metadata": {
        "id": "ovHr3xtIcHWe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#실행 예시\n",
        "\n",
        "pruning_ratios = [0.3]\n",
        "\n",
        "classifier_weights = torch.load('classifier.pt')  # 또는 model.classifier.state_dict()\n",
        "\n",
        "pruning_plan, new_channels, classifier_plan, is_valid = test_pruning_planner(\n",
        "    all_ranks=all_ranks,\n",
        "    pruning_ratios=pruning_ratios,\n",
        "    classifier_weights=classifier_weights,\n",
        "    classifier_ratio=0.3  # 30% 프루닝\n",
        ")\n"
      ],
      "metadata": {
        "id": "XsKCdtFCwvIp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##5. 프루닝 블록"
      ],
      "metadata": {
        "id": "2PB7Ekz0cOwC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#필요한 라이브러리\n",
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import MultiStepLR"
      ],
      "metadata": {
        "id": "U5SQ3BSYcQp2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 파인튜닝 용도 데이터셋\n",
        "train_data = torchvision.datasets.CIFAR100(root='./data', train=True, download=True, transform=torchvision.transforms.ToTensor())\n",
        "train_loader = torch.utils.data.DataLoader(train_data, batch_size=128, shuffle=True, num_workers=2)\n",
        "\n",
        "test_data = torchvision.datasets.CIFAR100(root='./data', train=False, download=True, transform=torchvision.transforms.ToTensor())\n",
        "test_loader = torch.utils.data.DataLoader(test_data, batch_size=128, shuffle=False, num_workers=2)\n",
        "print(\"데이터 준비 완료!\")"
      ],
      "metadata": {
        "id": "K4T1Ze4nxxad"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 프루닝,파인튜닝 함수"
      ],
      "metadata": {
        "id": "_9ffoIQc3irb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def prune_and_finetune_vgg16(orig_pt: str,\n",
        "                            pruning_plan: dict,\n",
        "                            train_dataloader,\n",
        "                            test_dataloader,\n",
        "                            num_classes=100,\n",
        "                            fine_tuning_epochs=30,\n",
        "                            initial_learning_rate=0.01,\n",
        "                            weight_decay=0.0005,\n",
        "                            milestones=[5, 10],\n",
        "                            gamma=0.1,\n",
        "                            device=None,\n",
        "                            classifier_plan=None):  # 새로 추가된 파라미터\n",
        "    \"\"\"\n",
        "    VGG16_BN 모델을 프루닝하고 fine-tune하는 함수\n",
        "\n",
        "    Args:\n",
        "        orig_pt: 원본 모델 체크포인트 경로\n",
        "        pruning_plan: 각 conv 레이어별 프루닝 정보 딕셔너리\n",
        "        train_dataloader: 훈련 데이터로더\n",
        "        test_dataloader: 테스트 데이터로더\n",
        "        num_classes: 클래스 수\n",
        "        fine_tuning_epochs: fine-tuning 에포크 수\n",
        "        initial_learning_rate: 초기 학습률\n",
        "        weight_decay: 가중치 감소\n",
        "        milestones: 학습률 스케줄링 마일스톤\n",
        "        gamma: 학습률 감소 비율\n",
        "        device: 디바이스\n",
        "        classifier_plan: classifier 프루닝 계획 (optional)\n",
        "    \"\"\"\n",
        "    device = device or torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    # VGG16_BN_PruneModel 인스턴스 생성\n",
        "    model = VGG16_BN_PruneModel(num_classes=num_classes).to(device)\n",
        "\n",
        "    # 체크포인트 로드 (features만)\n",
        "    ckpt = torch.load(orig_pt, map_location=device)\n",
        "    state_dict = ckpt['model_state_dict'] if 'model_state_dict' in ckpt else ckpt\n",
        "\n",
        "    # features 부분만 로드 (classifier는 nn.Identity()이므로 제외)\n",
        "    new_state_dict = {}\n",
        "    for k, v in state_dict.items():\n",
        "        if k.startswith('features.'):\n",
        "            new_state_dict[k] = v\n",
        "\n",
        "    model.load_state_dict(new_state_dict, strict=False)\n",
        "    print(\"원본 모델의 features 부분 로드 완료\")\n",
        "\n",
        "    # VGG16의 conv 레이어들 매핑 (MaxPool 제외하고 Conv2d만)\n",
        "    conv_indices = []\n",
        "    bn_indices = []\n",
        "\n",
        "    # features에서 Conv2d와 BatchNorm2d 레이어 인덱스 찾기\n",
        "    for i, layer in enumerate(model.features):\n",
        "        if isinstance(layer, nn.Conv2d):\n",
        "            conv_indices.append(i)\n",
        "            bn_indices.append(i + 1)  # Conv 다음이 BatchNorm\n",
        "\n",
        "    print(f\"Conv 레이어 인덱스: {conv_indices}\")\n",
        "    print(f\"BatchNorm 레이어 인덱스: {bn_indices}\")\n",
        "\n",
        "    print(\"프루닝 시작...\")\n",
        "\n",
        "    # 각 conv 레이어별로 프루닝 수행\n",
        "    for i, conv_name in enumerate([f'conv{j+1}' for j in range(len(conv_indices))]):\n",
        "        if conv_name not in pruning_plan:\n",
        "            print(f\"{conv_name}: 프루닝하지 않음\")\n",
        "            continue\n",
        "\n",
        "        conv_idx = conv_indices[i]\n",
        "        bn_idx = bn_indices[i]\n",
        "\n",
        "        # 현재 conv 레이어와 bn 레이어 가져오기\n",
        "        conv_layer = model.features[conv_idx]\n",
        "        bn_layer = model.features[bn_idx]\n",
        "\n",
        "        # pruning_plan에서 유지할 채널 인덱스 가져오기\n",
        "        keep_indices = pruning_plan[conv_name]['keep_indices']\n",
        "\n",
        "        print(f\"{conv_name}: {conv_layer.out_channels} -> {len(keep_indices)} 채널\")\n",
        "\n",
        "        # 1. 출력 채널 프루닝\n",
        "        old_conv = conv_layer\n",
        "        old_bn = bn_layer\n",
        "\n",
        "        # 새로운 conv 레이어 생성 (출력 채널 수 조정)\n",
        "        new_out_channels = len(keep_indices)\n",
        "        new_conv = nn.Conv2d(\n",
        "            old_conv.in_channels,\n",
        "            new_out_channels,\n",
        "            kernel_size=3,\n",
        "            padding=1,\n",
        "            bias=(old_conv.bias is not None)\n",
        "        )\n",
        "\n",
        "        # 가중치 복사 (유지할 채널들만)\n",
        "        new_conv.weight.data = old_conv.weight.data[keep_indices].clone()\n",
        "        if old_conv.bias is not None:\n",
        "            new_conv.bias.data = old_conv.bias.data[keep_indices].clone()\n",
        "\n",
        "        # 새로운 BatchNorm 레이어 생성\n",
        "        new_bn = nn.BatchNorm2d(new_out_channels)\n",
        "        new_bn.weight.data = old_bn.weight.data[keep_indices].clone()\n",
        "        new_bn.bias.data = old_bn.bias.data[keep_indices].clone()\n",
        "        new_bn.running_mean.data = old_bn.running_mean[keep_indices].clone()\n",
        "        new_bn.running_var.data = old_bn.running_var[keep_indices].clone()\n",
        "\n",
        "        # 레이어 교체\n",
        "        model.features[conv_idx] = new_conv.to(device)\n",
        "        model.features[bn_idx] = new_bn.to(device)\n",
        "\n",
        "        # 2. 다음 레이어의 입력 채널 조정 (마지막 레이어가 아닌 경우)\n",
        "        if i < len(conv_indices) - 1:\n",
        "            next_conv_idx = conv_indices[i + 1]\n",
        "            next_conv = model.features[next_conv_idx]\n",
        "\n",
        "            # 다음 conv 레이어의 입력 채널 조정\n",
        "            new_next_conv = nn.Conv2d(\n",
        "                new_out_channels,  # 현재 레이어의 출력이 다음 레이어의 입력\n",
        "                next_conv.out_channels,\n",
        "                kernel_size=3,\n",
        "                padding=1,\n",
        "                bias=(next_conv.bias is not None)\n",
        "            )\n",
        "\n",
        "            # 가중치 복사 (입력 채널 차원에서 유지할 채널들만)\n",
        "            new_next_conv.weight.data = next_conv.weight.data[:, keep_indices].clone()\n",
        "            if next_conv.bias is not None:\n",
        "                new_next_conv.bias.data = next_conv.bias.data.clone()\n",
        "\n",
        "            # 다음 레이어 교체는 다음 반복에서 처리됨 (출력 채널 프루닝 시)\n",
        "            # 여기서는 임시로 가중치만 업데이트\n",
        "            model.features[next_conv_idx] = new_next_conv.to(device)\n",
        "\n",
        "    print(\"프루닝 완료!\")\n",
        "\n",
        "    # 3. Classifier 재정의 (features의 출력 크기가 변했으므로)\n",
        "    # 더미 입력으로 features 출력 크기 계산\n",
        "    dummy_input = torch.randn(1, 3, 32, 32, device=device)  # CIFAR-10 크기로 가정\n",
        "    with torch.no_grad():\n",
        "        features_output = model.features(dummy_input)\n",
        "        flat_dim = features_output.view(1, -1).size(1)\n",
        "\n",
        "    # Classifier 생성 (프루닝 여부에 따라 다르게)\n",
        "    if classifier_plan is not None and len(classifier_plan.get('keep_indices', [])) > 0:\n",
        "        # Classifier 프루닝이 있는 경우\n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "        print(\"Classifier 프루닝 적용\")\n",
        "        print(\"=\"*60)\n",
        "\n",
        "        model.classifier = apply_classifier_pruning(\n",
        "            flat_dim=flat_dim,\n",
        "            hidden_dim=classifier_plan['original_hidden_dim'],\n",
        "            pruned_hidden_dim=classifier_plan['pruned_hidden_dim'],\n",
        "            keep_indices=classifier_plan['keep_indices'],\n",
        "            num_classes=num_classes,\n",
        "            orig_pt=orig_pt,\n",
        "            device=device\n",
        "        )\n",
        "    else:\n",
        "        # 기존 방식: 프루닝 없이 새로운 classifier 생성\n",
        "        model.classifier = nn.Sequential(\n",
        "            nn.Linear(flat_dim, 512),\n",
        "            nn.ReLU(True),\n",
        "            nn.Dropout(),\n",
        "            nn.Linear(512, 128),\n",
        "            nn.ReLU(True),\n",
        "            nn.Dropout(),\n",
        "            nn.Linear(128, num_classes)\n",
        "        ).to(device)\n",
        "\n",
        "    print(f\"새로운 classifier 입력 차원: {flat_dim}\")\n",
        "\n",
        "    # 4. Validation set 생성 (train 데이터의 10% 사용)\n",
        "    # sklearn 대신 torch의 random_split 사용\n",
        "\n",
        "    # train_dataloader의 데이터셋에서 인덱스 분할\n",
        "    dataset = train_dataloader.dataset\n",
        "    train_size = int(0.9 * len(dataset))\n",
        "    val_size = len(dataset) - train_size\n",
        "\n",
        "    train_subset, val_subset = torch.utils.data.random_split(\n",
        "        dataset, [train_size, val_size],\n",
        "        generator=torch.Generator().manual_seed(42)\n",
        "    )\n",
        "\n",
        "    train_loader_new = torch.utils.data.DataLoader(\n",
        "        train_subset, batch_size=train_dataloader.batch_size,\n",
        "        shuffle=True, num_workers=2\n",
        "    )\n",
        "    val_loader = torch.utils.data.DataLoader(\n",
        "        val_subset, batch_size=train_dataloader.batch_size,\n",
        "        shuffle=False, num_workers=2\n",
        "    )\n",
        "\n",
        "    print(f\"Train size: {len(train_subset)}, Validation size: {len(val_subset)}\")\n",
        "\n",
        "    # 4. Fine-tuning 설정\n",
        "    print(\"Fine-tuning 시작...\")\n",
        "\n",
        "    # SGD 옵티마이저 설정\n",
        "    optimizer = optim.SGD(\n",
        "        model.parameters(),\n",
        "        lr=initial_learning_rate,\n",
        "        weight_decay=weight_decay,\n",
        "        momentum=0.9  # SGD에 모멘텀 추가\n",
        "    )\n",
        "\n",
        "    # 학습률 스케줄러 설정\n",
        "    scheduler = MultiStepLR(optimizer, milestones=milestones, gamma=gamma)\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    # 기록용 리스트\n",
        "    train_losses, val_losses = [], []\n",
        "    train_accs, val_accs = [], []\n",
        "\n",
        "\n",
        "    # Fine-tuning 수행\n",
        "    for epoch in range(fine_tuning_epochs):\n",
        "        model.train()\n",
        "        train_loss = 0.0\n",
        "        train_correct = 0\n",
        "        train_total = 0\n",
        "\n",
        "        for batch_idx, (data, target) in enumerate(train_loader_new):\n",
        "            data, target = data.to(device), target.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            output = model(data)\n",
        "            loss = criterion(output, target)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            train_loss += loss.item()\n",
        "            _, predicted = torch.max(output.data, 1)\n",
        "            train_total += target.size(0)\n",
        "            train_correct += (predicted == target).sum().item()\n",
        "\n",
        "            if batch_idx % 100 == 0:\n",
        "                print(f'Epoch {epoch+1}/{fine_tuning_epochs}, '\n",
        "                      f'Batch {batch_idx}, '\n",
        "                      f'Loss: {loss.item():.6f}, '\n",
        "                      f'LR: {scheduler.get_last_lr()[0]:.6f}')\n",
        "\n",
        "        # Validation phase\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        val_correct = 0\n",
        "        val_total = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for data, target in val_loader:\n",
        "                data, target = data.to(device), target.to(device)\n",
        "                output = model(data)\n",
        "                loss = criterion(output, target)\n",
        "\n",
        "                val_loss += loss.item()\n",
        "                _, predicted = torch.max(output.data, 1)\n",
        "                val_total += target.size(0)\n",
        "                val_correct += (predicted == target).sum().item()\n",
        "\n",
        "        # 에포크별 결과 계산 및 저장\n",
        "        train_loss_avg = train_loss / len(train_loader_new)\n",
        "        val_loss_avg = val_loss / len(val_loader)\n",
        "        train_acc = 100 * train_correct / train_total\n",
        "        val_acc = 100 * val_correct / val_total\n",
        "\n",
        "        train_losses.append(train_loss_avg)\n",
        "        val_losses.append(val_loss_avg)\n",
        "        train_accs.append(train_acc)\n",
        "        val_accs.append(val_acc)\n",
        "\n",
        "        print(f'Epoch {epoch+1}: Train Loss: {train_loss_avg:.4f}, Train Acc: {train_acc:.2f}%')\n",
        "        print(f'           Val Loss: {val_loss_avg:.4f}, Val Acc: {val_acc:.2f}%')\n",
        "        print(f'           Gap: {train_acc - val_acc:.2f}%')\n",
        "\n",
        "        # 학습률 스케줄링\n",
        "        scheduler.step()\n",
        "\n",
        "    # 5. 평가\n",
        "    print(\"최종 평가 중...\")\n",
        "    model.eval()\n",
        "    test_loss = 0.0\n",
        "    test_correct = 0\n",
        "    test_total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_dataloader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            outputs = model(data)\n",
        "            # --- 추가 시작 ---\n",
        "            loss = criterion(outputs, target)\n",
        "            test_loss += loss.item()\n",
        "            # --- 추가 끝 ---\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            test_total += target.size(0)\n",
        "            test_correct += (predicted == target).sum().item()\n",
        "\n",
        "    test_loss_avg = test_loss / len(test_dataloader)\n",
        "    test_acc = 100 * test_correct / test_total\n",
        "    print(f'최종 테스트 결과: Loss: {test_loss_avg:.4f}, Accuracy: {test_acc:.2f}%')\n",
        "\n",
        "    # 6. 학습 곡선 플롯\n",
        "    import matplotlib.pyplot as plt\n",
        "\n",
        "    plt.figure(figsize=(12, 5))\n",
        "\n",
        "    # Loss plot\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(train_losses, label='Train Loss', color='blue')\n",
        "    plt.plot(val_losses, label='Validation Loss', color='orange')\n",
        "    plt.title('Loss Trend')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "\n",
        "    # Accuracy plot\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(train_accs, label='Train Accuracy', color='blue')\n",
        "    plt.plot(val_accs, label='Validation Accuracy', color='orange')\n",
        "    plt.title('Accuracy Trend')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Accuracy (%)')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Overfitting 분석\n",
        "    final_gap = train_accs[-1] - val_accs[-1]\n",
        "    if final_gap > 5:\n",
        "        print(f\"⚠️  Overfitting 감지: Train-Val gap = {final_gap:.2f}%\")\n",
        "    else:\n",
        "        print(f\"✅ 정상 학습: Train-Val gap = {final_gap:.2f}%\")\n",
        "\n",
        "    accuracy = test_acc\n",
        "\n",
        "\n",
        "    # 6. 모델 저장\n",
        "    torch.save({\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'pruning_plan': pruning_plan,\n",
        "        'accuracy': accuracy,\n",
        "        'training_config': {\n",
        "            'fine_tuning_epochs': fine_tuning_epochs,\n",
        "            'initial_learning_rate': initial_learning_rate,\n",
        "            'weight_decay': weight_decay,\n",
        "            'milestones': milestones,\n",
        "            'gamma': gamma\n",
        "        }\n",
        "    }, 'vgg16_pruned_finetuned.pth')\n",
        "\n",
        "    print(\"모델 저장 완료: vgg16_pruned_finetuned.pth\")\n",
        "\n",
        "    return model\n",
        "\n",
        "def apply_classifier_pruning(flat_dim: int,\n",
        "                            hidden_dim: int,\n",
        "                            pruned_hidden_dim: int,\n",
        "                            keep_indices: list,\n",
        "                            num_classes: int,\n",
        "                            orig_pt: str,\n",
        "                            device):\n",
        "    \"\"\"\n",
        "    Classifier 프루닝을 적용하여 새로운 classifier 생성\n",
        "\n",
        "    Args:\n",
        "        flat_dim: features 출력을 flatten한 차원\n",
        "        hidden_dim: 원본 hidden layer 차원\n",
        "        pruned_hidden_dim: 프루닝 후 hidden layer 차원\n",
        "        keep_indices: 유지할 뉴런 인덱스\n",
        "        num_classes: 클래스 수\n",
        "        orig_pt: 원본 모델 체크포인트 경로\n",
        "        device: 디바이스\n",
        "\n",
        "    Returns:\n",
        "        pruned_classifier: 프루닝된 classifier\n",
        "    \"\"\"\n",
        "    # 원본 classifier 가중치 로드\n",
        "    ckpt = torch.load(orig_pt, map_location=device)\n",
        "    state_dict = ckpt['model_state_dict'] if 'model_state_dict' in ckpt else ckpt\n",
        "\n",
        "    # classifier의 state_dict 추출\n",
        "    classifier_state = {}\n",
        "    for k, v in state_dict.items():\n",
        "        if k.startswith('classifier.'):\n",
        "            # 'classifier.' 접두사 제거\n",
        "            new_key = k.replace('classifier.', '')\n",
        "            classifier_state[new_key] = v\n",
        "\n",
        "    # 원본 가중치 가져오기\n",
        "    linear1_weight = classifier_state['linear1.weight']  # [hidden_dim, in_features]\n",
        "    linear1_bias = classifier_state['linear1.bias']      # [hidden_dim]\n",
        "    bn1_weight = classifier_state['norm1.weight']        # [hidden_dim]\n",
        "    bn1_bias = classifier_state['norm1.bias']            # [hidden_dim]\n",
        "    bn1_mean = classifier_state['norm1.running_mean']    # [hidden_dim]\n",
        "    bn1_var = classifier_state['norm1.running_var']      # [hidden_dim]\n",
        "    linear2_weight = classifier_state['linear2.weight']  # [num_classes, hidden_dim]\n",
        "    linear2_bias = classifier_state['linear2.bias']      # [num_classes]\n",
        "\n",
        "    keep_indices_tensor = torch.tensor(keep_indices, device=device)\n",
        "\n",
        "    # 프루닝된 classifier 생성\n",
        "    pruned_classifier = nn.Sequential(OrderedDict([\n",
        "        ('linear1', nn.Linear(flat_dim, pruned_hidden_dim)),\n",
        "        ('norm1', nn.BatchNorm1d(pruned_hidden_dim)),\n",
        "        ('relu1', nn.ReLU(inplace=True)),\n",
        "        ('dropout1', nn.Dropout(0.5)),\n",
        "        ('linear2', nn.Linear(pruned_hidden_dim, num_classes)),\n",
        "    ])).to(device)\n",
        "\n",
        "    # linear1 가중치 적용 (출력 뉴런만 프루닝)\n",
        "    # 입력 차원이 변했으므로 (features 프루닝 때문에) 초기화 후 부분 복사\n",
        "    with torch.no_grad():\n",
        "        # 원본 입력 차원과 새 입력 차원 비교\n",
        "        orig_in_features = linear1_weight.size(1)\n",
        "\n",
        "        if flat_dim == orig_in_features:\n",
        "            # 입력 차원이 같으면 그대로 복사\n",
        "            pruned_classifier.linear1.weight.data = linear1_weight[keep_indices_tensor].clone()\n",
        "        else:\n",
        "            # 입력 차원이 다르면 Xavier 초기화 (features가 프루닝되었을 경우)\n",
        "            print(f\"  경고: linear1 입력 차원 불일치 ({orig_in_features} -> {flat_dim}), 새로 초기화\")\n",
        "            nn.init.xavier_uniform_(pruned_classifier.linear1.weight)\n",
        "\n",
        "        pruned_classifier.linear1.bias.data = linear1_bias[keep_indices_tensor].clone()\n",
        "\n",
        "        # BatchNorm1d 가중치 적용\n",
        "        pruned_classifier.norm1.weight.data = bn1_weight[keep_indices_tensor].clone()\n",
        "        pruned_classifier.norm1.bias.data = bn1_bias[keep_indices_tensor].clone()\n",
        "        pruned_classifier.norm1.running_mean.data = bn1_mean[keep_indices_tensor].clone()\n",
        "        pruned_classifier.norm1.running_var.data = bn1_var[keep_indices_tensor].clone()\n",
        "\n",
        "        # linear2 가중치 적용 (입력 뉴런만 프루닝)\n",
        "        pruned_classifier.linear2.weight.data = linear2_weight[:, keep_indices_tensor].clone()\n",
        "        pruned_classifier.linear2.bias.data = linear2_bias.clone()\n",
        "\n",
        "    print(f\"  Classifier 프루닝 완료: {hidden_dim} -> {pruned_hidden_dim} hidden neurons\")\n",
        "    print(f\"  유지된 뉴런 인덱스 수: {len(keep_indices)}\")\n",
        "\n",
        "    return pruned_classifier"
      ],
      "metadata": {
        "id": "b9Zf1rauxgRx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 실행"
      ],
      "metadata": {
        "id": "T6Ngfdi44BCV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Conv + Classifier 함께 프루닝\n",
        "pruned_model = prune_and_finetune_vgg16(\n",
        "    orig_pt='model.pt',\n",
        "    pruning_plan=pruning_plan,\n",
        "    train_dataloader=train_loader,\n",
        "    test_dataloader=test_loader,\n",
        "    fine_tuning_epochs=30,\n",
        "    initial_learning_rate=0.007,\n",
        "    weight_decay=0.001,\n",
        "    milestones=[5, 10],\n",
        "    gamma=0.1,\n",
        "    classifier_plan=classifier_plan  # 추가!\n",
        ")"
      ],
      "metadata": {
        "id": "prSwUSsJ2sP9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 모델정보(프루닝 비율, 파라미터) 함수"
      ],
      "metadata": {
        "id": "5XkDEn3-4LbN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## 파라미터 계산, 모델정보 출력함수\n",
        "\n",
        "def get_model_info(model, show_classifier=True):\n",
        "    \"\"\"\n",
        "    프루닝된 모델의 정보를 출력하는 헬퍼 함수\n",
        "\n",
        "    Args:\n",
        "        model: PyTorch 모델\n",
        "        show_classifier: Classifier 정보 표시 여부\n",
        "    \"\"\"\n",
        "    total_params = sum(p.numel() for p in model.parameters())\n",
        "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "    print(\"=\"*70)\n",
        "    print(\"모델 정보\")\n",
        "    print(\"=\"*70)\n",
        "    print(f\"총 파라미터 수: {total_params:,}\")\n",
        "    print(f\"학습 가능한 파라미터 수: {trainable_params:,}\")\n",
        "\n",
        "    # Features 부분 파라미터 계산\n",
        "    features_params = sum(p.numel() for p in model.features.parameters())\n",
        "\n",
        "    # Classifier 부분 파라미터 계산\n",
        "    if hasattr(model, 'classifier') and not isinstance(model.classifier, nn.Identity):\n",
        "        classifier_params = sum(p.numel() for p in model.classifier.parameters())\n",
        "    else:\n",
        "        classifier_params = 0\n",
        "\n",
        "    print(f\"\\n파라미터 분포:\")\n",
        "    print(f\"  Features (Conv layers): {features_params:,} ({features_params/total_params*100:.1f}%)\")\n",
        "    print(f\"  Classifier: {classifier_params:,} ({classifier_params/total_params*100:.1f}%)\")\n",
        "\n",
        "    # 각 conv 레이어의 채널 수 출력\n",
        "    conv_count = 0\n",
        "    print(\"\\n각 Conv 레이어별 채널 수:\")\n",
        "    for i, layer in enumerate(model.features):\n",
        "        if isinstance(layer, nn.Conv2d):\n",
        "            conv_count += 1\n",
        "            params = layer.weight.numel() + (layer.bias.numel() if layer.bias is not None else 0)\n",
        "            print(f\"  Conv{conv_count}: {layer.in_channels:>4} -> {layer.out_channels:>4} \"\n",
        "                  f\"(파라미터: {params:,})\")\n",
        "\n",
        "    # Classifier 정보 출력\n",
        "    if show_classifier and hasattr(model, 'classifier') and not isinstance(model.classifier, nn.Identity):\n",
        "        print(\"\\nClassifier 구조:\")\n",
        "        for name, layer in model.classifier.named_children():\n",
        "            if isinstance(layer, nn.Linear):\n",
        "                params = layer.weight.numel() + (layer.bias.numel() if layer.bias is not None else 0)\n",
        "                print(f\"  {name}: {layer.in_features:>5} -> {layer.out_features:>5} \"\n",
        "                      f\"(파라미터: {params:,})\")\n",
        "            elif isinstance(layer, nn.BatchNorm1d):\n",
        "                params = layer.weight.numel() + layer.bias.numel()\n",
        "                print(f\"  {name}: {layer.num_features} features (파라미터: {params:,})\")\n",
        "\n",
        "\n",
        "def calculate_pruning_ratios(pruning_plan, classifier_plan=None, original_model=None, pruned_model=None):\n",
        "    \"\"\"\n",
        "    프루닝 비율 계산 및 출력\n",
        "\n",
        "    Args:\n",
        "        pruning_plan: Conv 레이어 프루닝 계획\n",
        "        classifier_plan: Classifier 프루닝 계획 (optional)\n",
        "        original_model: 원본 모델 (optional, 파라미터 감소량 계산용)\n",
        "        pruned_model: 프루닝된 모델 (optional, 파라미터 감소량 계산용)\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"프루닝 비율 정보\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    # Conv 레이어 프루닝 정보\n",
        "    print(\"\\n[Conv 레이어 프루닝]\")\n",
        "    total_original_channels = 0\n",
        "    total_pruned_channels = 0\n",
        "\n",
        "    for conv_name, info in pruning_plan.items():\n",
        "        original_channels = info['original_channels']\n",
        "        pruned_channels = info['pruned_channels']\n",
        "        ratio = info.get('pruning_ratio_actual', (original_channels - pruned_channels) / original_channels)\n",
        "\n",
        "        total_original_channels += original_channels\n",
        "        total_pruned_channels += pruned_channels\n",
        "\n",
        "        print(f\"  {conv_name}: {original_channels:>3} -> {pruned_channels:>3} \"\n",
        "              f\"(프루닝 비율: {ratio:>5.1%})\")\n",
        "\n",
        "    overall_conv_ratio = (total_original_channels - total_pruned_channels) / total_original_channels\n",
        "    print(f\"\\n  전체 Conv 채널 프루닝: {total_original_channels} -> {total_pruned_channels} \"\n",
        "          f\"({overall_conv_ratio:.1%})\")\n",
        "\n",
        "    # Classifier 프루닝 정보\n",
        "    if classifier_plan is not None and len(classifier_plan.get('keep_indices', [])) > 0:\n",
        "        print(\"\\n[Classifier 프루닝]\")\n",
        "        original_hidden = classifier_plan['original_hidden_dim']\n",
        "        pruned_hidden = classifier_plan['pruned_hidden_dim']\n",
        "        ratio = classifier_plan['pruning_ratio_actual']\n",
        "\n",
        "        print(f\"  Hidden Layer: {original_hidden:>5} -> {pruned_hidden:>5} neurons \"\n",
        "              f\"(프루닝 비율: {ratio:>5.1%})\")\n",
        "\n",
        "    # 전체 파라미터 감소량 계산\n",
        "    if original_model is not None and pruned_model is not None:\n",
        "        print(\"\\n\" + \"-\"*70)\n",
        "        print(\"[전체 파라미터 감소량]\")\n",
        "\n",
        "        # Features 파라미터\n",
        "        orig_features_params = sum(p.numel() for p in original_model.features.parameters())\n",
        "        pruned_features_params = sum(p.numel() for p in pruned_model.features.parameters())\n",
        "        features_reduction = (orig_features_params - pruned_features_params) / orig_features_params\n",
        "\n",
        "        print(f\"  Features: {orig_features_params:,} -> {pruned_features_params:,} \"\n",
        "              f\"({features_reduction:.1%} 감소)\")\n",
        "\n",
        "        # Classifier 파라미터\n",
        "        if hasattr(original_model, 'classifier') and hasattr(pruned_model, 'classifier'):\n",
        "            if not isinstance(original_model.classifier, nn.Identity):\n",
        "                orig_classifier_params = sum(p.numel() for p in original_model.classifier.parameters())\n",
        "                pruned_classifier_params = sum(p.numel() for p in pruned_model.classifier.parameters())\n",
        "                classifier_reduction = (orig_classifier_params - pruned_classifier_params) / orig_classifier_params\n",
        "\n",
        "                print(f\"  Classifier: {orig_classifier_params:,} -> {pruned_classifier_params:,} \"\n",
        "                      f\"({classifier_reduction:.1%} 감소)\")\n",
        "\n",
        "        # 전체\n",
        "        orig_total_params = sum(p.numel() for p in original_model.parameters())\n",
        "        pruned_total_params = sum(p.numel() for p in pruned_model.parameters())\n",
        "        total_reduction = (orig_total_params - pruned_total_params) / orig_total_params\n",
        "\n",
        "        print(f\"\\n  전체: {orig_total_params:,} -> {pruned_total_params:,} \"\n",
        "              f\"({total_reduction:.1%} 감소)\")\n",
        "        print(f\"  압축률: {pruned_total_params/orig_total_params:.2%} (원본의 {pruned_total_params/orig_total_params:.1%})\")\n",
        "\n",
        "\n",
        "# 사용 예시\n",
        "\"\"\"\n",
        "# 1. 프루닝 전 모델 정보\n",
        "print(\"원본 모델:\")\n",
        "get_model_info(original_model)\n",
        "\n",
        "# 2. 프루닝 후 모델 정보\n",
        "print(\"\\n프루닝된 모델:\")\n",
        "get_model_info(pruned_model)\n",
        "\n",
        "# 3. 프루닝 비율 상세 정보 (Conv만)\n",
        "calculate_pruning_ratios(pruning_plan)\n",
        "\n",
        "# 4. 프루닝 비율 상세 정보 (Conv + Classifier)\n",
        "calculate_pruning_ratios(\n",
        "    pruning_plan=pruning_plan,\n",
        "    classifier_plan=classifier_plan,\n",
        "    original_model=original_model,\n",
        "    pruned_model=pruned_model\n",
        ")\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "xSWe8Mxc133m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# 1. 프루닝 전 모델 정보\n",
        "print(\"원본 모델:\")\n",
        "get_model_info(original_model)\n",
        "\n",
        "# 2. 프루닝 후 모델 정보\n",
        "print(\"\\n프루닝된 모델:\")\n",
        "get_model_info(pruned_model)\n",
        "\n",
        "# 4. 프루닝 비율 상세 정보 (Conv + Classifier)\n",
        "calculate_pruning_ratios(\n",
        "    pruning_plan=pruning_plan,\n",
        "    classifier_plan=classifier_plan,\n",
        "    original_model=original_model,\n",
        "    pruned_model=pruned_model\n",
        ")\n"
      ],
      "metadata": {
        "id": "I-Fa_jIz2JbP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##6. 시각자료 모음"
      ],
      "metadata": {
        "id": "KwbQsB82dyWU"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cspXjaxidzwM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}